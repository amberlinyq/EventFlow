# ============================================
# EventFlow Environment Configuration
# ============================================
# INSTRUCTIONS:
# 1. Copy this file to .env: cp env.template .env
# 2. Replace ALL placeholder values marked with <<< >>> below
# 3. Never commit .env to git (it's in .gitignore)
# ============================================

# Database Configuration
# <<< REPLACE THIS >>> - Your PostgreSQL connection string
# Format: postgresql://username:password@host:port/database?schema=public
# Example: postgresql://postgres:mypassword@localhost:5432/eventflow?schema=public
# For Cloud SQL: postgresql://user:pass@/dbname?host=/cloudsql/project:region:instance
DATABASE_URL="postgresql://<<<USERNAME>>>:<<<PASSWORD>>>@<<<HOST>>>:<<<PORT>>>/<<<DATABASE>>>?schema=public"

# Server Configuration
PORT=8080
NODE_ENV=development

# Google Cloud Platform Configuration
# <<< REPLACE THIS >>> - Your GCP Project ID (found in GCP Console)
GCP_PROJECT_ID="<<<YOUR-GCP-PROJECT-ID>>>"

# Pub/Sub Configuration
# These can stay as defaults, or customize
PUBSUB_TOPIC_NAME=events
PUBSUB_SUBSCRIPTION_NAME=events-worker

# GCP Credentials
# <<< REPLACE THIS >>> - Path to your GCP service account JSON key file
# 1. Go to: https://console.cloud.google.com/iam-admin/serviceaccounts
# 2. Create or select a service account
# 3. Create a key (JSON format) and download it
# 4. Place it in the project root and update the path below
# Example: ./gcp-credentials.json
GCP_CREDENTIALS_PATH=./<<<gcp-credentials.json>>>

# BigQuery Configuration
# <<< OPTIONAL - Can use defaults >>> - BigQuery dataset and table names
# The dataset and table will be created automatically if they don't exist
# Make sure your GCP service account has BigQuery Data Editor and BigQuery Job User roles
BIGQUERY_DATASET_ID=eventflow
BIGQUERY_TABLE_ID=events

# Logging Configuration
# Options: fatal, error, warn, info, debug, trace
LOG_LEVEL=info

